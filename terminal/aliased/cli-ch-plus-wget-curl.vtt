WEBVTT

1
00:00:01.792 --> 00:00:03.439
<v Instructor>So looking at two tools here.</v>

2
00:00:03.439 --> 00:00:06.282
One is W get, and one is curl.

3
00:00:06.282 --> 00:00:10.518
Now both of them are used to connect to the Internet

4
00:00:10.518 --> 00:00:13.523
or connect over HTTP and grab URLs,

5
00:00:13.523 --> 00:00:16.488
but they are used for different things.

6
00:00:16.488 --> 00:00:19.118
And you might find that one's installed on your machine,

7
00:00:19.118 --> 00:00:20.931
and one isn't.

8
00:00:20.931 --> 00:00:22.372
I can never remember which is the most common.

9
00:00:22.372 --> 00:00:25.011
I think it's curl, and certainly,

10
00:00:25.011 --> 00:00:27.276
from my perspective, I use curl a lot more

11
00:00:27.276 --> 00:00:28.693
than I use W get.

12
00:00:30.363 --> 00:00:34.530
Now W get is really handy for grabbing single web pages.

13
00:00:35.592 --> 00:00:38.759
So here I've got W get, remysharp.com.

14
00:00:39.993 --> 00:00:41.429
I don't even need the protocol.

15
00:00:41.429 --> 00:00:44.094
And it will go off to my blog

16
00:00:44.094 --> 00:00:47.430
and just grab the page it finds on the other end

17
00:00:47.430 --> 00:00:48.710
and save it to disc.

18
00:00:48.710 --> 00:00:51.135
So you can see now, I have index.html,

19
00:00:51.135 --> 00:00:52.970
and if I opened that up

20
00:00:52.970 --> 00:00:56.970
in a browser you would see that that is my blog.

21
00:00:58.118 --> 00:01:02.285
My index page of my blog, and it saved it with no trouble.

22
00:01:03.304 --> 00:01:07.050
And the same thing with kind of redirects as well,

23
00:01:07.050 --> 00:01:11.129
to have a magic redirect, where it takes the short part

24
00:01:11.129 --> 00:01:13.518
of a blog post and then matches it

25
00:01:13.518 --> 00:01:16.643
to the closest match of W get node.

26
00:01:16.643 --> 00:01:20.676
It redirects correctly and finds the right file

27
00:01:20.676 --> 00:01:21.791
and saves it.

28
00:01:21.791 --> 00:01:25.081
So we can see up here, we might see kind of a, yeah,

29
00:01:25.081 --> 00:01:26.605
following, here we go.

30
00:01:26.605 --> 00:01:28.989
So it's following the redirect for us.

31
00:01:28.989 --> 00:01:30.757
It's really good for grabbing single files.

32
00:01:30.757 --> 00:01:32.237
Maybe that's perfect for what you want.

33
00:01:32.237 --> 00:01:33.511
And if you need to instal it,

34
00:01:33.511 --> 00:01:35.585
you'll need to do a per instal, possibly,

35
00:01:35.585 --> 00:01:37.699
I can't remember.

36
00:01:37.699 --> 00:01:40.459
But also it's very good for spidering websites.

37
00:01:40.459 --> 00:01:43.406
So if you've got a website you want to take a copy of,

38
00:01:43.406 --> 00:01:45.529
W get is really good for spidering.

39
00:01:45.529 --> 00:01:48.254
So, I'll show you that.

40
00:01:48.254 --> 00:01:50.250
Bear in mind I Googled the syntax for this.

41
00:01:50.250 --> 00:01:52.534
I wouldn't expect you to memorise it.

42
00:01:52.534 --> 00:01:55.951
So here we're saying to follow the links,

43
00:01:56.884 --> 00:02:00.884
link -L4, for is four links deep, as it spiders,

44
00:02:02.032 --> 00:02:05.039
go ahead and use spider argument.

45
00:02:05.039 --> 00:02:08.330
<v ->D means the the domain's to stay within</v>

46
00:02:08.330 --> 00:02:09.871
when it's following and spidering off,

47
00:02:09.871 --> 00:02:13.408
so don't go on to Google or anything like that,

48
00:02:13.408 --> 00:02:14.426
just stay within that domain,

49
00:02:14.426 --> 00:02:15.889
and that can be multiple domains.

50
00:02:15.889 --> 00:02:17.514
They kick that off.

51
00:02:17.514 --> 00:02:19.089
You just saw it grabbing the URLs

52
00:02:19.089 --> 00:02:21.124
as quickly as it possibly can.

53
00:02:21.124 --> 00:02:24.694
I'm just gonna kill that early, and if we see,

54
00:02:24.694 --> 00:02:27.906
I've got this new directory called remysharp.com.

55
00:02:27.906 --> 00:02:31.406
And if I just do a quick find, oops, find,

56
00:02:34.112 --> 00:02:37.491
you can see here's all the files it managed to grab.

57
00:02:37.491 --> 00:02:39.804
It didn't get very far because I killed it early,

58
00:02:39.804 --> 00:02:42.408
but you can see it's gone all the way back,

59
00:02:42.408 --> 00:02:45.075
it's grabbed all the HTML files,

60
00:02:46.886 --> 00:02:47.736
and if you leave that running,

61
00:02:47.736 --> 00:02:50.821
it would do a complete spider.

62
00:02:50.821 --> 00:02:54.265
So, that's what W get is good for.

63
00:02:54.265 --> 00:02:56.932
Like I said, I use a curl a lot.

64
00:02:57.961 --> 00:03:02.128
Curl is similar to W get, but it's a little bit stricter.

65
00:03:03.044 --> 00:03:05.961
So I've had to put protocol in here

66
00:03:07.920 --> 00:03:10.970
when I'm curling remysharp.com.

67
00:03:10.970 --> 00:03:13.692
And when I run it, the output actually gets put

68
00:03:13.692 --> 00:03:15.808
onto the terminal.

69
00:03:15.808 --> 00:03:17.353
So if we wanted to save that into a file,

70
00:03:17.353 --> 00:03:21.378
I would do index.html, and that would be the way I save it.

71
00:03:21.378 --> 00:03:23.897
But, because it gets printed as standard out,

72
00:03:23.897 --> 00:03:26.296
so that the terminal, I can actually pipe it into something.

73
00:03:26.296 --> 00:03:30.296
So I can, I have a little command called scrape,

74
00:03:31.947 --> 00:03:35.154
and I can scrape all the P tags.

75
00:03:35.154 --> 00:03:37.107
So that gives me all the P tags that are

76
00:03:37.107 --> 00:03:38.854
in that index page.

77
00:03:38.854 --> 00:03:41.531
And because it can be piped, it can be piped

78
00:03:41.531 --> 00:03:43.563
to other tools like grep.

79
00:03:43.563 --> 00:03:45.480
I can grep for the H-1.

80
00:03:46.491 --> 00:03:50.074
Oops, and I can see, there's a lot of H-1s.

81
00:03:51.581 --> 00:03:55.164
Nope, that's a, I don't know what that is,

82
00:03:56.867 --> 00:03:59.388
anyway, I can grep for the H-1s,

83
00:03:59.388 --> 00:04:02.431
or grep for whatever I'm looking for in the HTML,

84
00:04:02.431 --> 00:04:03.527
and it will give me that part.

85
00:04:03.527 --> 00:04:05.059
Whereas the W get will get will give me the file,

86
00:04:05.059 --> 00:04:07.216
so I have to go looking for it.

87
00:04:07.216 --> 00:04:09.732
Also, with curl, you have a lot more control,

88
00:04:09.732 --> 00:04:11.113
lot more fine grain control.

89
00:04:11.113 --> 00:04:16.026
So when I did curl remysharp.com/node,

90
00:04:16.026 --> 00:04:17.855
this doesn't do anything, and the reason,

91
00:04:17.855 --> 00:04:21.309
it does do something, but it's printing

92
00:04:21.309 --> 00:04:23.181
what the server gave back to me,

93
00:04:23.181 --> 00:04:25.414
and what the server gave back to me was no content,

94
00:04:25.414 --> 00:04:26.595
just headers.

95
00:04:26.595 --> 00:04:28.020
So if I want to look at the headers,

96
00:04:28.020 --> 00:04:29.020
I can do -I.

97
00:04:30.582 --> 00:04:32.819
And here, I can see actually, the server responded

98
00:04:32.819 --> 00:04:36.317
with a 301 and a location redirect.

99
00:04:36.317 --> 00:04:39.690
And because I only told curl to grab that URL,

100
00:04:39.690 --> 00:04:43.105
the output, the body of that request is empty.

101
00:04:43.105 --> 00:04:45.421
But I can tell it to follow a redirect.

102
00:04:45.421 --> 00:04:49.588
So I can do -L, and it will follow the redirect correctly

103
00:04:50.785 --> 00:04:52.289
and get me the index page.

104
00:04:52.289 --> 00:04:54.359
And if I just want to look at the headers for that,

105
00:04:54.359 --> 00:04:57.764
I can do -I, and you can see it following all

106
00:04:57.764 --> 00:04:58.597
of those redirects.

107
00:04:58.597 --> 00:05:00.613
So the first one is to move it to HTTPS,

108
00:05:00.613 --> 00:05:03.379
the second one is to say I've found the actual blog post,

109
00:05:03.379 --> 00:05:05.943
and this last one's saying not found,

110
00:05:05.943 --> 00:05:06.937
the reason it says "not found" is

111
00:05:06.937 --> 00:05:10.391
because I've used -I, and what -I says

112
00:05:10.391 --> 00:05:12.466
is do a head request.

113
00:05:12.466 --> 00:05:14.700
So there's different types of requests.

114
00:05:14.700 --> 00:05:19.578
You've got HEAD, we have GET, we have POST, DELETE,

115
00:05:19.578 --> 00:05:21.043
PUT and so on.

116
00:05:21.043 --> 00:05:23.876
HEAD says to only get the headers.

117
00:05:24.787 --> 00:05:26.856
I need to say get me everything.

118
00:05:26.856 --> 00:05:29.215
So I need to do a GET request.

119
00:05:29.215 --> 00:05:31.982
And that would do the 200 OK,

120
00:05:31.982 --> 00:05:35.968
and if I didn't have the capital I, I would see the body.

121
00:05:35.968 --> 00:05:38.854
If I wanted headers and the body, I can do small I,

122
00:05:38.854 --> 00:05:40.642
and give me the headers and the body,

123
00:05:40.642 --> 00:05:43.642
so I scroll back past all this gunk,

124
00:05:44.829 --> 00:05:48.489
and should see here, yep there we go,

125
00:05:48.489 --> 00:05:51.376
so we've got headers, and then we have the actual body

126
00:05:51.376 --> 00:05:52.554
that follows.

127
00:05:52.554 --> 00:05:54.946
Really good for kind of diagnosing things like

128
00:05:54.946 --> 00:05:56.776
making sure cookies there,

129
00:05:56.776 --> 00:05:58.771
making sure the redirects are happening,

130
00:05:58.771 --> 00:06:01.981
making sure that you can see cloudflare is there,

131
00:06:01.981 --> 00:06:04.898
I think there should be some cache,

132
00:06:06.779 --> 00:06:11.012
it should say whether it was hit or miss.

133
00:06:11.012 --> 00:06:13.262
There's something in there.

134
00:06:14.145 --> 00:06:15.161
And then there's a few other things,

135
00:06:15.161 --> 00:06:18.659
like I said, I can do curl -X POST,

136
00:06:18.659 --> 00:06:20.895
so I can post data up to and endpoint,

137
00:06:20.895 --> 00:06:23.984
so I can do, I'm gonna do jsonbin,

138
00:06:23.984 --> 00:06:25.896
but it's gonna give me a fail.

139
00:06:25.896 --> 00:06:29.896
I'm gonna do data is, let's say, an empty array,

140
00:06:34.000 --> 00:06:38.167
/me/test, and that should give me 401, yeah,

141
00:06:39.967 --> 00:06:42.246
it's trying to redirect that's telling me I'm not allowed.

142
00:06:42.246 --> 00:06:45.791
So I can look at the headers on that, as well, -I,

143
00:06:45.791 --> 00:06:47.383
so I can do some debugging on the command line.

144
00:06:47.383 --> 00:06:50.474
401 unauthorize, while it trying to redirect me.

145
00:06:50.474 --> 00:06:54.171
I can post real data info if I was preauthorize

146
00:06:54.171 --> 00:06:58.338
I could send extra headers, so I do -H, authorization,

147
00:07:00.067 --> 00:07:02.094
token, I think I've got environment,

148
00:07:02.094 --> 00:07:06.261
Jsonbin TOKEN, nope need to do that in double quotes,

149
00:07:10.394 --> 00:07:12.790
and we should see if that works.

150
00:07:12.790 --> 00:07:16.623
So we've got 201 and that said it was created.

151
00:07:17.467 --> 00:07:19.132
So it managed to make this thing for me.

152
00:07:19.132 --> 00:07:21.965
I can do PUTs, DELETEs, and so on.

153
00:07:22.993 --> 00:07:26.743
And something that's also really useful to me

154
00:07:28.694 --> 00:07:31.746
is in DEV tools, if you look at the network panel,

155
00:07:31.746 --> 00:07:33.250
and you actually have the network request,

156
00:07:33.250 --> 00:07:35.814
if you right-click on the individual network requests,

157
00:07:35.814 --> 00:07:38.375
you can copy them as curl.

158
00:07:38.375 --> 00:07:40.780
So you can copy all as curl, every single request.

159
00:07:40.780 --> 00:07:42.489
You can copy that single one as a curl.

160
00:07:42.489 --> 00:07:44.890
So I just got this Time To First Byte Wikipedia page.

161
00:07:44.890 --> 00:07:46.188
I'm going to do copy as curl,

162
00:07:46.188 --> 00:07:49.414
go back to my terminal, and just paste.

163
00:07:49.414 --> 00:07:51.746
And there you can see the full curl command.

164
00:07:51.746 --> 00:07:54.425
I'm just gonna do -I, so I just look up headers.

165
00:07:54.425 --> 00:07:55.481
And these are the headers coming back

166
00:07:55.481 --> 00:07:56.987
from the Wikipedia page.

167
00:07:56.987 --> 00:08:00.644
And it was called exactly the same way my browser called.

168
00:08:00.644 --> 00:08:03.244
So if I had any cookies, verisign in,

169
00:08:03.244 --> 00:08:05.478
all of that would go to the curl request.

170
00:08:05.478 --> 00:08:07.264
And it would get a, it would replicate

171
00:08:07.264 --> 00:08:09.217
the entire request properly.

172
00:08:09.217 --> 00:08:11.620
So again, really good for debugging.

173
00:08:11.620 --> 00:08:14.453
And there's one last thing.

174
00:08:14.453 --> 00:08:16.370
I have this alias perf.

175
00:08:18.648 --> 00:08:21.987
Perf is aliased to curl, and this command.

176
00:08:21.987 --> 00:08:26.049
So when I do perf, it runs that command,

177
00:08:26.049 --> 00:08:29.011
and then I put in my URL like this.

178
00:08:29.011 --> 00:08:33.116
And what it would do is give me the performance metrics

179
00:08:33.116 --> 00:08:35.949
of the request for this URL, okay?

180
00:08:36.819 --> 00:08:38.570
So here it tells me it took a total

181
00:08:38.570 --> 00:08:42.356
of 200 and, sorry, 280 milliseconds,

182
00:08:42.356 --> 00:08:44.837
the appconnect, I think, is the time to first byte

183
00:08:44.837 --> 00:08:47.241
or pretransfer, one of these two is the time to first byte,

184
00:08:47.241 --> 00:08:48.219
and that's the interesting part.

185
00:08:48.219 --> 00:08:51.792
That's the part that says the DNS was resolved

186
00:08:51.792 --> 00:08:54.842
and the server is starting to send data back down

187
00:08:54.842 --> 00:08:56.470
over the wire.

188
00:08:56.470 --> 00:09:00.447
The way this works is firstly, I have this -O,

189
00:09:00.447 --> 00:09:02.475
this says don't print any output.

190
00:09:02.475 --> 00:09:06.296
<v ->O says "take the output and send it to this,"</v>

191
00:09:06.296 --> 00:09:09.552
and dev/null means basic complete suppress.

192
00:09:09.552 --> 00:09:12.686
<v ->S gets rid of the kind of progress that it might show,</v>

193
00:09:12.686 --> 00:09:15.655
and -W is the thing that says

194
00:09:15.655 --> 00:09:18.419
to print the performance stats.

195
00:09:18.419 --> 00:09:21.428
Now I have this file in my home directory,

196
00:09:21.428 --> 00:09:23.178
so if I cat that out,

197
00:09:24.806 --> 00:09:27.733
which uses special variables that curl understands

198
00:09:27.733 --> 00:09:30.169
and it will print out in this format for me.

199
00:09:30.169 --> 00:09:34.336
And the at symbol says read, read it from this file.

200
00:09:35.943 --> 00:09:37.241
So yeah, really useful.

201
00:09:37.241 --> 00:09:40.207
There's loads that you can learn about curl and W get,

202
00:09:40.207 --> 00:09:42.685
but I want to show you the two are linked

203
00:09:42.685 --> 00:09:45.196
to examples in this video, as well.

204
00:09:45.196 --> 00:09:47.188
But, like I said, my preference is curl

205
00:09:47.188 --> 00:09:49.219
for, kind of, that fine grain control.

206
00:09:49.219 --> 00:09:53.198
But W get is really useful for spidering entire websites.

